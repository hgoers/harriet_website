[
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Teaching",
    "section": "",
    "text": "An introduction to research methods and quantitative research in political science.\n\n\n\n\n\n\n\n\n\n\n\n\nLearn to use data science tools to support your research."
  },
  {
    "objectID": "teaching/index.html#fall-2024-and-2023",
    "href": "teaching/index.html#fall-2024-and-2023",
    "title": "Teaching",
    "section": "",
    "text": "An introduction to research methods and quantitative research in political science.\n\n\n\n\n\n\n\n\n\n\n\n\nLearn to use data science tools to support your research."
  },
  {
    "objectID": "teaching/index.html#spring-2024",
    "href": "teaching/index.html#spring-2024",
    "title": "Teaching",
    "section": "Spring 2024",
    "text": "Spring 2024\n\n\n\n\n\n\n\n\n\nGVPT722: Advanced Quantitative Methods for Political Science\nAn introduction to multivariate analysis."
  },
  {
    "objectID": "blog/intro_to_ERGM.html",
    "href": "blog/intro_to_ERGM.html",
    "title": "Introduction to Exponential Random Graph Models",
    "section": "",
    "text": "set.seed(1234)\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "blog/intro_to_ERGM.html#what-is-an-ergm",
    "href": "blog/intro_to_ERGM.html#what-is-an-ergm",
    "title": "Introduction to Exponential Random Graph Models",
    "section": "What is an ERGM?",
    "text": "What is an ERGM?\nImagine you observe some network — for example, countries at war with each other. You want to learn why this network looks the way it does. An ERGM provides the following answer: “Some network patterns are more likely than others, and I can explain why by looking at their specific features”."
  },
  {
    "objectID": "blog/intro_to_ERGM.html#why-networks-and-ergms-matter-for-political-science",
    "href": "blog/intro_to_ERGM.html#why-networks-and-ergms-matter-for-political-science",
    "title": "Introduction to Exponential Random Graph Models",
    "section": "Why Networks and ERGMs Matter for Political Science",
    "text": "Why Networks and ERGMs Matter for Political Science\nOne core problem with using standard statistical methods, like regression, to study connections between countries, organizations, or people is that these methods assume each relationship is independent of every other. For instance, they treat the relationship between the US and France as if it exists in isolation, completely unaffected by the US’s relationship with Germany. This assumption is fundamentally inaccurate in real-world political and social systems, where ties influence each other.\nExponential Random Graph Models (ERGMs) are a specialized set of statistical techniques designed to analyze networks by recognizing and modeling these complex interdependencies. They look for and quantify common patterns that emerge when relationships influence one another, such as reciprocity (if A links to B, B is likely to link back to A) or transitivity (if A links to B, and B links to C, then A is likely to link to C). This allows researchers to study how the overall structure of the network, not just individual factors, drives the formation of new connections. By doing this, ERGMs offer a more accurate and realistic framework than methods that treat all connections as standalone events.\n\nThe Basic Logic\nStart with all possible networks that could form between your actors. For example, if you have three actors (all of which can both send and receive ties), 64 directed networks could form between them.\n\n\n\n\n\nNext, assign each of these possible networks some probability based on its features. For example, imagine that each of the above networks represents a different configuration of who is at war with whom. We might find that:\n\nNetworks where democracies fight other democracies might be LESS likely (democratic peace theory)\nNetworks where geographically distant countries fight might be LESS likely\nNetworks where strong countries attack weak ones might be MORE likely\n\nNetworks with more “desirable” features receive a higher probability. Those with less “desirable” features receive a lower probability. The parameters (β values) tell you how much each feature matters."
  },
  {
    "objectID": "blog/intro_to_ERGM.html#predicting-international-conflict",
    "href": "blog/intro_to_ERGM.html#predicting-international-conflict",
    "title": "Introduction to Exponential Random Graph Models",
    "section": "Predicting International Conflict",
    "text": "Predicting International Conflict\nLet’s work through a concrete example. Imagine you would like to work out what features shape the likelihood of war breaking out. You know the following features matter:\n\nDemocracy: Whether two countries are both democracies (if yes, they are less likely to go to war with each other)\nDistance: The distance between their capital cities (war becomes less likely with distance)\nMilitary strength: Their relative strength (stronger countries are more likely to attack weaker ones)\nAvoiding multi-front wars: If A fights B, and B fights C, then A is less likely to also fight C (countries avoid complex triangular conflicts and instead form coalitions)\n\nWe will examine a hypothetical world of three countries in which these four feature are the only significant determinants of one country starting a war with another. Further, we will know just how much each feature matters to this decision. Knowing this allows us to see how ERGMs work. We can then move on to the more useful research project of discovering these parameters.\n\n\n\n\n\n\nWhy Three Nodes?\n\n\n\nThe number of possible networks grows exponentially with the number of nodes. With three nodes, we have a manageable 64 possible networks that we can enumerate completely. This lets us understand the mechanics of ERGMs before tackling larger networks that require simulation methods.\n\n\n\nStep 1: Define Node Attributes\nLet’s set up our known world of three countries:\n\nnodes &lt;- tibble(\n  node = c(\"A\", \"B\", \"C\"),\n  democracy = c(1, 1, 0),     # A and B are democracies, C is not\n  cinc = c(.142, .07, .04)    # Composite Index of National Capability\n)\n\nknitr::kable(nodes)\n\n\n\n\nnode\ndemocracy\ncinc\n\n\n\n\nA\n1\n0.142\n\n\nB\n1\n0.070\n\n\nC\n0\n0.040\n\n\n\n\n\nWhat do these variables mean?\n\ndemocracy: A value of 1 means the country is a democracy, 0 means it is not. In our world, countries A and B are democracies, while C is not.\ncinc: This is the Composite Index of National Capability, which measures a country’s military power. It ranges from 0 to 1, with higher values indicating greater military capability. In our example, A is the strongest (0.142), followed by B (0.07), then C (0.04).\n\n\n\nStep 2: Create All Possible Directed Ties (Dyads)\nFor a three-node directed network, we have 3 × 2 = 6 possible ties. Each of our three nodes can send a tie (initiate war) to two other nodes.\n\ndyads &lt;- expand_grid(\n  node_1 = nodes$node,\n  node_2 = nodes$node\n) %&gt;%\n  filter(node_1 != node_2) %&gt;%  # Remove self-loops\n  left_join(rename_with(nodes, ~ paste0(.x, \"_1\")), \n            by = \"node_1\") %&gt;%\n  left_join(rename_with(nodes, ~ paste0(.x, \"_2\")), \n            by = \"node_2\") %&gt;%\n  transmute(\n    node_1, node_2,\n    \n    # Covariate 1: Both nodes are democracies\n    both_democracy = as.numeric(democracy_1 == 1 & democracy_2 == 1),\n    \n    # Covariate 2: CINC ratio (sender/receiver)\n    cinc_ratio = cinc_1 / cinc_2,\n    \n    # Covariate 3: Distance between capitals in thousands of kms\n    dist = c(12, 40, 12, 35, 40, 35)\n  )\n\nknitr::kable(dyads)\n\n\n\n\nnode_1\nnode_2\nboth_democracy\ncinc_ratio\ndist\n\n\n\n\nA\nB\n1\n2.0285714\n12\n\n\nA\nC\n0\n3.5500000\n40\n\n\nB\nA\n1\n0.4929577\n12\n\n\nB\nC\n0\n1.7500000\n35\n\n\nC\nA\n0\n0.2816901\n40\n\n\nC\nB\n0\n0.5714286\n35\n\n\n\n\n\nUnderstanding the covariates:\n\nboth_democracy: This equals 1 if both node_1 and node_2 are democracies. In our example, this is true for A→B and B→A. We expect these dyads to be less likely to go to war.\ncinc_ratio: This is the military capability of node_1 divided by node_2. Values greater than 1 mean the potential attacker is stronger. For example, A→C has a ratio of 3.55, meaning A is much stronger than C.\ndist: Distance between capitals in thousands of kilometers. Greater distances should make war less likely.\n\n\n\nStep 3: Define the “True” ERGM Parameters\nWe’re going to set up a known world where we specify exactly how each feature affects the probability of war. In real research, we would estimate these parameters from data, but here we’ll define them to understand how ERGMs work.\n\ntrue_beta_0 &lt;- -2.0   # Baseline\ntrue_beta_1 &lt;- -3.5   # Both democracy effect  \ntrue_beta_2 &lt;- 1.5    # CINC ratio effect\ntrue_beta_3 &lt;- -0.05  # Distance effect\ntrue_beta_4 &lt;- -2.0   # Transitive triads effect\n\nInterpreting each parameter:\n\nβ₀ = -2.0 (Baseline): This negative number means wars are generally rare. If we ignore all other features, countries don’t randomly fight — there needs to be a reason.\nβ₁ = -3.5 (Both Democracy): This large negative number means when both countries are democracies, war becomes much less likely. This captures the democratic peace theory.\nβ₂ = 1.5 (CINC Ratio): This positive number means stronger countries are more likely to attack weaker ones. Higher military advantage increases the probability of war.\nβ₃ = -0.05 (Distance): This negative number means farther countries are less likely to fight. Geography creates a barrier to conflict.\nβ₄ = -2.0 (Transitive Triads): This is our structural parameter. It captures patterns of conflict clustering, which we’ll explain next.\n\n\n\nUnderstanding the Structural Feature: Transitive Triads\nERGMs allow us to capture features that shape the likelihood of some outcome other than those that depend only on the two countries involved in a potential war (A and B’s attributes, their distance, etc.). For example, the transitive triads parameter is different — it depends on the entire network structure. We cannot use traditional dyadic data structures to estimate its influence on conflict.\nWhat is a transitive triad?\nA transitive triad exists when three countries form a directed triangle of conflict:\n\nA is at war with B (A→B exists)\nB is at war with C (B→C exists)\nA is at war with C (A→C exists)\n\nThis forms a complete cycle where each country that fights another country also fights that country’s enemy.\nWhy does this matter?\nIn reality, transitive triads are rare in international conflict. Here’s why:\n\nIf the United States fights Russia, and Russia fights Ukraine, the United States typically does not fight Ukraine. Instead, the U.S. often supports Ukraine (the principle: “the enemy of my enemy is my friend”).\nCountries tend to form coalitions against common enemies rather than creating triangular wars. When A fights B, and B fights C, A and C more often become allies than enemies.\n\nThe key insight: With β₄ = -2.0 (negative), networks that contain transitive triads are less probable. Countries avoid getting into complex multi-front wars. This one parameter changes the entire distribution of likely networks by discouraging clustered conflict patterns.\n\n\nStep 4: Calculate Network Probabilities\nHere’s where ERGMs with structural features become different. We cannot simply calculate the probability of each tie independently. Why? Because whether A→C exists depends on whether A→B and B→C exist!\nThe solution: We must look at all 64 possible complete networks and calculate the probability of each entire network configuration.\n\nGenerate All Possible Networks\n\nall_networks &lt;- expand_grid(\n  tie_AB = 0:1, tie_AC = 0:1, tie_BA = 0:1,\n  tie_BC = 0:1, tie_CA = 0:1, tie_CB = 0:1\n) %&gt;%\n  mutate(network_id = row_number())\n\nall_networks\n\n# A tibble: 64 × 7\n   tie_AB tie_AC tie_BA tie_BC tie_CA tie_CB network_id\n    &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;      &lt;int&gt;\n 1      0      0      0      0      0      0          1\n 2      0      0      0      0      0      1          2\n 3      0      0      0      0      1      0          3\n 4      0      0      0      0      1      1          4\n 5      0      0      0      1      0      0          5\n 6      0      0      0      1      0      1          6\n 7      0      0      0      1      1      0          7\n 8      0      0      0      1      1      1          8\n 9      0      0      1      0      0      0          9\n10      0      0      1      0      0      1         10\n# ℹ 54 more rows\n\n\nEach row represents one possible network. For example:\n\nRow 1: tie_AB = 0, tie_AC = 0, ..., tie_CB = 0 (empty network, no wars)\nRow 64: tie_AB = 1, tie_AC = 1, ..., tie_CB = 1 (complete network, all at war)\n\n\n\nCalculate Sufficient Statistics for Each Network\nFor each possible network, we need to count:\n\nHow many wars exist (edges)\nHow many wars involve two democracies\nSum of CINC ratios across all wars\nSum of distances across all wars\nHow many transitive triads exist (the structural feature)\n\nWhat are sufficient statistics?\nSufficient statistics are summary measures of a network that contain all the information needed to calculate the network’s probability under an ERGM.\nRather than tracking every individual tie separately, sufficient statistics reduce the network to a few key counts that capture what the model cares about. These five statistics are “sufficient” because two networks with identical sufficient statistics have identical probabilities under the model, even if the specific ties differ.\nWhy “sufficient”?\nFor example, suppose:\n\nNetwork A has wars A→B and C→D (two wars between non-democracies)\nNetwork B has wars A→C and B→D (two different wars between non-democracies)\n\nIf both networks have the same sufficient statistics (same number of edges, same democracy pairs, same total CINC ratios, same total distances, same transitive triads), they receive the same probability—the model only cares about the aggregate counts, not which specific countries fight which others.\nNow let’s calculate these statistics:\n\nnetwork_stats &lt;- all_networks %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    # Sufficient statistic 1: Number of edges (wars)\n    n_edges = sum(c(tie_AB, tie_AC, tie_BA, tie_BC, tie_CA, tie_CB)),\n    \n    # Sufficient statistic 2: Wars where both are democracies\n    n_both_dem = tie_AB * dyads$both_democracy[1] +  # A→B\n                 tie_BA * dyads$both_democracy[3],   # B→A\n    \n    # Sufficient statistic 3: Sum of CINC ratios\n    sum_cinc = tie_AB * dyads$cinc_ratio[1] +  # A→B\n               tie_AC * dyads$cinc_ratio[2] +  # A→C\n               tie_BA * dyads$cinc_ratio[3] +  # B→A\n               tie_BC * dyads$cinc_ratio[4] +  # B→C\n               tie_CA * dyads$cinc_ratio[5] +  # C→A\n               tie_CB * dyads$cinc_ratio[6],   # C→B\n    \n    # Sufficient statistic 4: Sum of distances\n    sum_dist = tie_AB * dyads$dist[1] +\n               tie_AC * dyads$dist[2] +\n               tie_BA * dyads$dist[3] +\n               tie_BC * dyads$dist[4] +\n               tie_CA * dyads$dist[5] +\n               tie_CB * dyads$dist[6],\n    \n    # Sufficient statistic 5: Number of transitive triads\n    # Count patterns where i→j AND j→k AND i→k all exist\n    n_transitive = (tie_AB * tie_BC * tie_AC) +  # A→B→C (and A→C)\n                   (tie_AC * tie_CB * tie_AB) +  # A→C→B (and A→B)\n                   (tie_BA * tie_AC * tie_BC) +  # B→A→C (and B→C)\n                   (tie_BC * tie_CA * tie_BA) +  # B→C→A (and B→A)\n                   (tie_CA * tie_AB * tie_CB) +  # C→A→B (and C→B)\n                   (tie_CB * tie_BA * tie_CA)    # C→B→A (and C→A)\n  ) %&gt;%\n  ungroup()\n\nselect(network_stats, network_id:n_transitive)\n\n# A tibble: 64 × 6\n   network_id n_edges n_both_dem sum_cinc sum_dist n_transitive\n        &lt;int&gt;   &lt;int&gt;      &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;        &lt;int&gt;\n 1          1       0          0    0            0            0\n 2          2       1          0    0.571       35            0\n 3          3       1          0    0.282       40            0\n 4          4       2          0    0.853       75            0\n 5          5       1          0    1.75        35            0\n 6          6       2          0    2.32        70            0\n 7          7       2          0    2.03        75            0\n 8          8       3          0    2.60       110            0\n 9          9       1          1    0.493       12            0\n10         10       2          1    1.06        47            0\n# ℹ 54 more rows\n\n\n\n\nCalculate Network Probabilities Using the ERGM Formula\nThe probability of each network is given by:\n\\[P(\\text{Network} \\mid \\theta) = \\frac{\\exp(\\theta' \\cdot s(\\text{Network}))}{\\kappa(\\theta)}\\]\nWhere:\n\n\\(\\theta\\) = our parameters (β₀, β₁, β₂, β₃, β₄)\n\\(s(\\text{Network})\\) = sufficient statistics for that network\n\\(\\kappa(\\theta)\\) = normalizing constant (ensures probabilities sum to 1 by being the sum of all network scores)\n\nIn plain language: “The probability of a network equals its score divided by the sum of all possible networks’ scores.”\n\n# Calculate the unnormalized log-probability for each network\nnetwork_stats &lt;- network_stats %&gt;%\n  mutate(\n    log_prob_unnorm = true_beta_0 * n_edges +\n                      true_beta_1 * n_both_dem +\n                      true_beta_2 * sum_cinc +\n                      true_beta_3 * sum_dist +\n                      true_beta_4 * n_transitive\n  )\n\n# Calculate the normalizing constant\nlog_kappa &lt;- log(sum(exp(network_stats$log_prob_unnorm)))\n\n# Calculate final probabilities\nnetwork_stats &lt;- network_stats %&gt;%\n  mutate(\n    log_prob = log_prob_unnorm - log_kappa,\n    prob = exp(log_prob)\n  ) %&gt;%\n  arrange(desc(prob))\n\nselect(network_stats, network_id, log_prob_unnorm, log_prob, prob)\n\n# A tibble: 64 × 4\n   network_id log_prob_unnorm log_prob    prob\n        &lt;int&gt;           &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n 1         17           1.32    -0.639 0.528  \n 2         21           0.200   -1.76  0.171  \n 3          1           0       -1.96  0.140  \n 4          5          -1.12    -3.09  0.0455 \n 5         18          -1.57    -3.53  0.0292 \n 6         49          -1.73    -3.70  0.0248 \n 7         19          -2.25    -4.22  0.0147 \n 8         22          -2.69    -4.66  0.00949\n 9          2          -2.89    -4.86  0.00777\n10         33          -3.06    -5.02  0.00659\n# ℹ 54 more rows\n\n\n\n\n\n\n\n\nUnderstanding the log space calculations\n\n\n\nYou might wonder why we use log_prob = log_prob_unnorm - log_kappa instead of directly calculating probabilities. Here’s why:\nThe mathematical identity:\n\nWe want: P = exp(score) / κ(θ)\nTaking logs: log(P) = log(exp(score)) - log(κ(θ)) = score - log(κ(θ))\n\nWhy work in log space?\n\nNetwork probabilities can be extremely small (like 0.0000001)\nComputers can’t reliably store very small numbers (they “underflow” to exactly 0)\nBut computers can store log(0.0000001) = -16.1 reliably\nWe only convert back to regular probabilities at the end: prob = exp(log_prob)\n\nThe calculation steps:\n\nlog_prob_unnorm = the score for each network (sum of β × statistics)\nlog_kappa = log of the sum of all exponential scores\nlog_prob = normalized log-probability = score - log(normalizing constant)\nprob = convert back to regular probability scale\n\n\n\nVerify probabilities sum to 1:\n\nsum(network_stats$prob)\n\n[1] 1\n\n\nPerfect! This confirms our probability distribution is valid.\n\n\n\nStep 5: Examine the Most Likely Networks\n\nnetwork_stats %&gt;%\n  select(network_id, n_edges, n_both_dem, n_transitive, sum_cinc, sum_dist, \n         prob) %&gt;%\n  slice(1:10) %&gt;%\n  mutate(prob_pct = scales::percent(prob, accuracy = 0.01)) %&gt;%\n  select(-prob) %&gt;%\n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\nnetwork_id\nn_edges\nn_both_dem\nn_transitive\nsum_cinc\nsum_dist\nprob_pct\n\n\n\n\n17\n1\n0\n0\n3.5500000\n40\n52.76%\n\n\n21\n2\n0\n0\n5.3000000\n75\n17.13%\n\n\n1\n0\n0\n0\n0.0000000\n0\n14.02%\n\n\n5\n1\n0\n0\n1.7500000\n35\n4.55%\n\n\n18\n2\n0\n0\n4.1214286\n75\n2.92%\n\n\n49\n2\n1\n0\n5.5785714\n52\n2.48%\n\n\n19\n2\n0\n0\n3.8316901\n80\n1.47%\n\n\n22\n3\n0\n0\n5.8714286\n110\n0.95%\n\n\n2\n1\n0\n0\n0.5714286\n35\n0.78%\n\n\n33\n1\n1\n0\n2.0285714\n12\n0.66%\n\n\n\n\n\nWhat do we notice?\nThe most probable networks tend to have:\n\nFew edges (wars are rare, β₀ is negative)\nLow or zero democracy pairs fighting (β₁ is very negative)\nFew or no transitive triads (countries avoid multi-front wars, β₄ is negative)\nWhen wars occur, they tend to be between strong and weak countries at close distances\n\n\n\nStep 6: Simulate a Network from This Distribution\nNow that we have probabilities for all 64 networks, we can randomly sample one network according to these probabilities. This is like the “data generating process” — it shows us what networks we’d expect to observe given our parameters.\n\n# Sample one network according to probabilities\nsampled_network_id &lt;- sample(network_stats$network_id, \n                              size = 1, \n                              prob = network_stats$prob)\n\nsampled_network &lt;- network_stats %&gt;%\n  filter(network_id == sampled_network_id)\n\nOur sampled network:\n\nsampled_network %&gt;% \n  select(network_id:n_transitive, prob) %&gt;% \n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\nnetwork_id\nn_edges\nn_both_dem\nsum_cinc\nsum_dist\nn_transitive\nprob\n\n\n\n\n17\n1\n0\n3.55\n40\n0\n0.5276205\n\n\n\n\n\nWhich specific wars exist?\n\nties_list &lt;- sampled_network %&gt;%\n  select(tie_AB, tie_AC, tie_BA, tie_BC, tie_CA, tie_CB) %&gt;%\n  pivot_longer(everything(), names_to = \"tie\", values_to = \"exists\") %&gt;%\n  filter(exists == 1) %&gt;%\n  mutate(tie = str_replace(tie, \"tie_\", \"\"),\n         tie = paste0(str_sub(tie, 1, 1), \"→\", str_sub(tie, 2, 2)))\n\nif (nrow(ties_list) &gt; 0) {\n  cat(\"Wars in this network:\\n\")\n  print(ties_list$tie)\n} else {\n  cat(\"Empty network - no wars\\n\")\n}\n\nWars in this network:\n[1] \"A→C\""
  },
  {
    "objectID": "blog/intro_to_ERGM.html#understanding-the-impact-of-network-structure",
    "href": "blog/intro_to_ERGM.html#understanding-the-impact-of-network-structure",
    "title": "Introduction to Exponential Random Graph Models",
    "section": "Understanding the Impact of Network Structure",
    "text": "Understanding the Impact of Network Structure\n\nWhat Does the Transitivity Parameter Mean?\nOur parameter β₄ = -2.0 means that each additional transitive triad multiplies the probability of a network by \\(\\exp(-2.0) \\approx 0.14\\) (or divides it by about 7.4).\nConcrete example:\nImagine two networks that are identical in every way except:\n\nNetwork 1 has 0 transitive triads\nNetwork 2 has 1 transitive triad\n\nNetwork 2 is only 14% as probable as Network 1! Or equivalently, Network 1 is 7.4 (1 / 0.14) times more probable than Network 2.\n\n\nComparing With and Without Structure\nLet’s see how the structural parameter changes things. We’ll calculate probabilities of each network with and without the transitivity term.\n\n# Calculate probabilities WITHOUT transitivity\nnetwork_stats_no_struct &lt;- network_stats %&gt;%\n  mutate(\n    log_prob_unnorm_no_struct = true_beta_0 * n_edges +\n                                true_beta_1 * n_both_dem +\n                                true_beta_2 * sum_cinc +\n                                true_beta_3 * sum_dist\n                                # Note: NO β₄ term!\n  )\n\nlog_kappa_no_struct &lt;- log(sum(exp(network_stats_no_struct$log_prob_unnorm_no_struct)))\n\nnetwork_stats_no_struct &lt;- network_stats_no_struct %&gt;%\n  mutate(\n    log_prob_no_struct = log_prob_unnorm_no_struct - log_kappa_no_struct,\n    prob_no_struct = exp(log_prob_no_struct)\n  )\n\nTop 10 networks WITH transitivity:\n\nnetwork_stats %&gt;% \n  select(network_id, n_edges, n_transitive, prob) %&gt;% \n  slice(1:10) %&gt;%\n  mutate(prob = scales::percent(prob, accuracy = 0.01)) %&gt;%\n  knitr::kable()\n\n\n\n\nnetwork_id\nn_edges\nn_transitive\nprob\n\n\n\n\n17\n1\n0\n52.76%\n\n\n21\n2\n0\n17.13%\n\n\n1\n0\n0\n14.02%\n\n\n5\n1\n0\n4.55%\n\n\n18\n2\n0\n2.92%\n\n\n49\n2\n0\n2.48%\n\n\n19\n2\n0\n1.47%\n\n\n22\n3\n0\n0.95%\n\n\n2\n1\n0\n0.78%\n\n\n33\n1\n0\n0.66%\n\n\n\n\n\nTop 10 networks WITHOUT transitivity:\n\nnetwork_stats_no_struct %&gt;% \n  arrange(desc(prob_no_struct)) %&gt;%\n  select(network_id, n_edges, n_transitive, prob_no_struct) %&gt;% \n  slice(1:10) %&gt;%\n  mutate(prob_no_struct = scales::percent(prob_no_struct, accuracy = 0.01)) %&gt;%\n  knitr::kable()\n\n\n\n\nnetwork_id\nn_edges\nn_transitive\nprob_no_struct\n\n\n\n\n17\n1\n0\n52.26%\n\n\n21\n2\n0\n16.97%\n\n\n1\n0\n0\n13.89%\n\n\n5\n1\n0\n4.51%\n\n\n18\n2\n0\n2.90%\n\n\n49\n2\n0\n2.46%\n\n\n19\n2\n0\n1.46%\n\n\n22\n3\n0\n0.94%\n\n\n53\n3\n1\n0.80%\n\n\n2\n1\n0\n0.77%\n\n\n\n\n\nKey observation: When we include the transitivity parameter (with its negative value), networks with fewer transitive triads become more probable. The structural feature reshapes the probability distribution to discourage complex conflict triangles!"
  },
  {
    "objectID": "blog/intro_to_ERGM.html#why-this-matters-interdependence",
    "href": "blog/intro_to_ERGM.html#why-this-matters-interdependence",
    "title": "Introduction to Exponential Random Graph Models",
    "section": "Why This Matters: Interdependence",
    "text": "Why This Matters: Interdependence\nThe crucial insight is that with structural parameters, we can no longer think about ties in isolation.\nWithout structure (dyad-independent model):\n\nWe could say: “The probability that A attacks B is 15%”\nEach tie’s probability is independent of other ties\n\nWith structure (our model):\n\nWe cannot say “P(A→B) = 15%” in isolation\nThe probability of A→B depends on whether A→C, B→C, etc. exist\nWe must think about the probability of entire network configurations\n\nThis interdependence captures real-world coalition dynamics where:\n\nCountries avoid multi-front wars\nShared enemies create opportunities for alliance\nTransitive conflict patterns (A fights B, B fights C, A fights C) are strategically undesirable\nThe structure of existing conflicts shapes future conflict decisions"
  },
  {
    "objectID": "blog/intro_to_ERGM.html#the-computational-challenge",
    "href": "blog/intro_to_ERGM.html#the-computational-challenge",
    "title": "Introduction to Exponential Random Graph Models",
    "section": "The Computational Challenge",
    "text": "The Computational Challenge\nFor our 3-node network, we enumerated all 64 possible networks. But this approach doesn’t scale:\n\n3 nodes: \\(2^6 = 64\\) networks (easy!)\n10 nodes: \\(2^{90} \\approx 1.2 \\times 10^{27}\\) networks (impossible!)\n100 nodes: \\(2^{9900}\\) networks (beyond astronomical)\n\nFor larger networks, we cannot enumerate all possibilities. Instead, researchers use Markov Chain Monte Carlo (MCMC) simulation to estimate ERGM parameters, which we will step through in the next post. But the core logic remains the same: we’re modeling a probability distribution over all possible networks, with parameters that make certain structural patterns more or less likely."
  },
  {
    "objectID": "blog/intro_to_ERGM.html#summary",
    "href": "blog/intro_to_ERGM.html#summary",
    "title": "Introduction to Exponential Random Graph Models",
    "section": "Summary",
    "text": "Summary\nERGMs let us answer: “Why does this network look the way it does?” by:\n\nDefining a probability distribution over all possible networks\nUsing parameters to weight different features (democracy, distance, strength, structure)\nFinding which networks are most probable given those parameters\n\nStructural parameters (such as transitivity) are what makes ERGMs powerful — they let us capture how the presence of some ties makes other ties more or less likely. In our case, the negative transitivity parameter reflects a fundamental strategic principle: countries avoid getting into wars with both sides of an existing conflict, leading to coalition formation rather than conflict clustering."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "These posts include notes on my current research and reminders of how to do things in R.\n\n\n\n\n\n\n\n\n\nFitting ERGMs to Real Networks\n\n\nUsing MCMC Simulation for Congressional Co-Sponsorship\n\n\n\nERGM\n\nNetwork analysis\n\nMCMC\n\nSimulation\n\n\n\n\n\n\nNov 19, 2025\n\n\n\n\n\n\n\nIntroduction to Exponential Random Graph Models\n\n\nUnderstanding Network Structure with the Enemy’s Enemy Principle\n\n\n\nERGM\n\nNetwork analysis\n\n\n\n\n\n\nNov 18, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research/ml_coding.html",
    "href": "research/ml_coding.html",
    "title": "ML-Assisted Events Data Coding",
    "section": "",
    "text": "I am working on a number of projects to help teams of researchers incorporate machine-learning models into their events data coding. I have had the most success using GPT models to help locate relevant news articles and to extract relevant details from those relevant news articles.\nI have also produced a number of Shiny applications that help speed up human coding or verification of ML-models’ outputs and reduce human error.\n\nWorking papers\nCoding with the machines: machine-assisted coding of rare event data, with Johanna Birnir, Ernesto Calvo, Keith Chew, Jordan Dewar, Roman Hlatky, Amy Liu, Henry Overos, and Ojashwi Pathak.\nWhile machine coding of data has dramatically advanced in recent years, we do not know the relative performance of supervised and semi-supervised algorithms when coding political data – especially when compared to human coding. In this note, we juxtapose (1) a semi-supervised dictionary-based classifier with (2) an out-of-the-box GPT 4 model and (3) a fine-tuned BERT transformer – and compare all three to human coding of rare events data. We find that the transformer models outperform the bag-of-words approach, especially when fine tuned. None of the models beat human coding – but only in well-known contexts. Our results, however, suggest that the transformers do better than humans in less familiar contexts and that with additional training they can rapidly close the remaining gap to humans. We conclude by discussing the benefits and drawbacks of machine coding.\n\n\nPresentations\n\nUMD Methods Workshop (Fall 2023)\nDo you research political events, including protests, mediation, voting, or sanctions? Our traditional method for collecting these events data involves reading thousands of news articles to extract the relevant information and put it into a dataset that we can use for our analysis. This process is very time-consuming and can be prone to error. Happily, advanced large language models, including GPT, can assist us. These models are easy to use in R and can be seamlessly integrated into your data collection process. I will step through how to use out-of-the-box GPT models in R to extract useful information from news articles. You can use these models to extract information from thousands of news articles in mere minutes."
  },
  {
    "objectID": "research/trade_dependence.html",
    "href": "research/trade_dependence.html",
    "title": "Trade Dependence",
    "section": "",
    "text": "I am interested in exploring how trade dependence between states changes their behaviour. I am currently focused on developing a new measure of bilateral trade dependence that better reflects the costs states face when trade disruptions are threatened.\n\nWorking Papers\nPlenty of Fish in the Sea: A New Approach to Measuring Trade Dependence"
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Research",
    "section": "",
    "text": "I am interested in dependence between countries and its effects on non-state actors. I am particularly interested in the effect of dependence on conflict outcomes."
  },
  {
    "objectID": "research/index.html#published-and-working-papers",
    "href": "research/index.html#published-and-working-papers",
    "title": "Research",
    "section": "Published and working papers",
    "text": "Published and working papers\n\nInternal drivers of self-rule referendums (2024)\nIn Conflict Management and Peace Science\nWith Kathleen Gallagher Cunningham, and Laia Balcells\nFrom Catalonia to Kurdistan to Scotland, referendums have increasingly become popular strategies of self-rule movements. Despite this, many referendums are considered failures by the movements (revealing a dearth of support), some are marred by violence, and few garner external backing. Given this, when are they likely to be employed? We argue that internal competition serves as one driving force for actors to use referendums as a way to gain or uphold status within the movement. Using novel data and two case studies, we highlight the ways these movements arrive at a vote for self-rule, underscoring the role of internal competition.\n\n\nCoding with the machines: machine-assisted coding of rare event data (2024)\nIn Proceedings of the National Academy of Sciences of the United States of America\nWith Henry David Overos, Roman Hlatky, Ojashwi Pathak, Jordan Gouws-Dewar, Katy Smith, Keith Padraic Chew, Jóhanna K Birnir, and Amy H Liu\nWhile machine coding of data has dramatically advanced in recent years, the literature raises significant concerns about validation of LLM classification showing, for example, that reliability varies greatly by prompt and temperature tuning, across subject areas and tasks—especially in “zero-shot” applications. This paper contributes to the discussion of validation in several different ways. To test the relative performance of supervised and semi-supervised algorithms when coding political data, we compare three models’ performances to each other over multiple iterations for each model and to trained expert coding of data. We also examine changes in performance resulting from prompt engineering and pre-processing of source data. To ameliorate concerns regarding LLM’s pre-training on test data, we assess performance by updating an existing dataset beyond what is publicly available. Overall, we find that only GPT-4 approaches trained expert coders when coding contexts familiar to human coders and codes more consistently across contexts. We conclude by discussing some benefits and drawbacks of machine coding moving forward.\n\n\nPlenty of Fish in the Sea (working paper)\nThis paper examines how how third-party states’ ability to fill trade gaps left by economic sanctions influences whether sanctions are imposed in the first place. It argues that sanctions are less likely when the target state can easily find alternative markets for the specific goods being restricted, as this undermines the sanctions’ effectiveness. By focusing on product-specific trade networks, rather than broad economic ties, the study offers a more nuanced understanding of when and why sanctions are used. This research is important as it sheds light on why some states evade sanctions and highlights the importance of global trade networks in shaping economic statecraft. The findings have significant implications for how states wield sanctions to achieve geopolitical goals in an interconnected world.\n\n\nTactics of Survival: Strategies of Resistance Data Project Update (forthcoming)\nWith Kathleen Cunningham, Ted Ellsworth, Michael Cowan, Oja Pathak, and Ellin Chung\nThis data feature presents the updated Strategies of Resistance Datas Project (SRDP), which provides organization-level data on duration of claim-making activity, violent and nonviolent tactics, demands over self-determination, and accommodations by the state. In the original data release, key comparisons were drawn between uses of mass nonviolence and smaller-scale nonviolent tactics, highlighting how common nonviolent direct action is, the extent to which it co-occurs with violence, and what we miss by examining only mass nonviolent campaigns. In the update, which includes new organizational-level data to 2020, we demonstrate how these data can be leveraged to understand and uniquely test hypotheses about organizational survival. Evaluating a set of hypotheses about the correlates of organizational survival drawn from the literature, we find that nonviolent tactical diversity is associated with longer survival and accommodation of the self-determination movements is associated with quicker exit."
  },
  {
    "objectID": "research/index.html#research-areas",
    "href": "research/index.html#research-areas",
    "title": "Research",
    "section": "Research areas",
    "text": "Research areas"
  },
  {
    "objectID": "blog/ERGMs_simulation.html",
    "href": "blog/ERGMs_simulation.html",
    "title": "Fitting ERGMs to Real Networks",
    "section": "",
    "text": "library(tidyverse)\nlibrary(network)\nlibrary(ergm)\n\nset.seed(12345)"
  },
  {
    "objectID": "blog/ERGMs_simulation.html#the-computational-challenge",
    "href": "blog/ERGMs_simulation.html#the-computational-challenge",
    "title": "Fitting ERGMs to Real Networks",
    "section": "The Computational Challenge",
    "text": "The Computational Challenge\nIn the previous post, we worked with a 3-node network where we could enumerate all 64 possible networks. We calculated the probability of each one, and then randomly sampled one according to those probabilities.\nBut what happens when we study real political networks?\n\nU.S. Senate (100 senators): \\(2^{4950}\\) possible networks (that’s a 1 followed by 1,490 zeros!)\nU.S. House (435 representatives): \\(2^{94395}\\) possible networks (incomprehensibly large)\nState legislature (even 50 members): \\(2^{1225}\\) possible networks\n\nWe cannot enumerate all possible networks. The numbers are astronomically large. We need a different approach: simulation."
  },
  {
    "objectID": "blog/ERGMs_simulation.html#our-example-congressional-co-sponsorship",
    "href": "blog/ERGMs_simulation.html#our-example-congressional-co-sponsorship",
    "title": "Fitting ERGMs to Real Networks",
    "section": "Our Example: Congressional Co-Sponsorship",
    "text": "Our Example: Congressional Co-Sponsorship\nLet’s study how members of Congress collaborate on legislation. When Representative Smith co-sponsors Representative Jones’ bill, they form a tie. This creates a network of legislative collaboration.\nResearch questions we might ask:\n\nDo members of the same party co-sponsor together more often?\nIs co-sponsorship reciprocal? (If A co-sponsors B’s bills, does B co-sponsor A’s?)\nDo legislators form tight collaboration clusters (triangles of mutual support)?\nDoes seniority matter for attracting co-sponsors?\n\nWe’ll use a synthetic dataset of 20 House members to demonstrate how to fit ERGMs when enumeration is impossible."
  },
  {
    "objectID": "blog/ERGMs_simulation.html#step-1-load-and-examine-the-network",
    "href": "blog/ERGMs_simulation.html#step-1-load-and-examine-the-network",
    "title": "Fitting ERGMs to Real Networks",
    "section": "Step 1: Load and Examine the Network",
    "text": "Step 1: Load and Examine the Network\nLet’s create a realistic co-sponsorship network for 20 House members:\n\n# Create a network of 20 House members\nn_members &lt;- 20\n\n# Node attributes\nmember_data &lt;- tibble(\n  name = paste0(\"Rep_\", 1:n_members),\n  party = rep(c(\"D\", \"R\"), each = n_members/2),  # 10 Democrats, 10 Republicans\n  seniority = sample(1:20, n_members, replace = FALSE)  # Years in Congress\n)\n\n# Create the network object\nhouse_net &lt;- network(n_members, directed = TRUE)\n\n# Add vertex attributes\nset.vertex.attribute(house_net, \"party\", member_data$party)\nset.vertex.attribute(house_net, \"seniority\", member_data$seniority)\nset.vertex.attribute(house_net, \"name\", member_data$name)\n\n# Simulate some realistic co-sponsorship ties\n# We'll create ties based on party and reciprocity\nset.seed(12345)\nfor(i in 1:n_members) {\n  for(j in 1:n_members) {\n    if(i != j) {\n      party_match &lt;- member_data$party[i] == member_data$party[j]\n      # Higher probability if same party\n      prob &lt;- ifelse(party_match, 0.25, 0.05)\n      if(runif(1) &lt; prob) {\n        house_net[i, j] &lt;- 1\n      }\n    }\n  }\n}\n\nUnderstanding our network:\n\n20 nodes: Each represents a House member\nDirected ties: If Rep_1 → Rep_2, it means Rep_1 co-sponsors Rep_2’s bills\nAttributes: Each member has a party (D or R) and seniority (years in Congress)\n\n\n# Visualize the network\nplot(house_net, \n     vertex.col = ifelse(member_data$party == \"D\", \"steelblue\", \"darkred\"),\n     vertex.cex = member_data$seniority / 5,\n     main = \"House Co-Sponsorship Network\\n(Blue = Democrat, Red = Republican)\",\n     edge.col = rgb(0, 0, 0, 0.3))\n\n\n\n\n\n\n\n\nWhat do we see?\nMembers of the same party (same color) appear to co-sponsor together more often. In fact, by design, members of the same party have a 25% chance of co-sponsoring bills together. Members of opposite parties have only 5% chance of doing so. But we need ERGMs to rigorously identify and test this, and account for other factors."
  },
  {
    "objectID": "blog/ERGMs_simulation.html#step-2-the-impossible-task-of-enumeration",
    "href": "blog/ERGMs_simulation.html#step-2-the-impossible-task-of-enumeration",
    "title": "Fitting ERGMs to Real Networks",
    "section": "Step 2: The Impossible Task of Enumeration",
    "text": "Step 2: The Impossible Task of Enumeration\nFor our 20-member network, there are \\(2^{380}\\) possible networks (20 × 19 = 380 possible directed ties).\nThe fundamental problem: We cannot calculate the normalizing constant κ(θ) by summing over all possible networks because there are too many.\nThe solution: Instead of calculating κ(θ) exactly, we estimate it using simulation."
  },
  {
    "objectID": "blog/ERGMs_simulation.html#step-3-understanding-mcmc-simulation",
    "href": "blog/ERGMs_simulation.html#step-3-understanding-mcmc-simulation",
    "title": "Fitting ERGMs to Real Networks",
    "section": "Step 3: Understanding MCMC Simulation",
    "text": "Step 3: Understanding MCMC Simulation\nMCMC stands for “Markov Chain Monte Carlo.” Let’s break down what this means.\nThe core idea:\nInstead of looking at ALL possible networks (impossible), we’ll:\n\nStart with our observed network\nMake small random changes (add a tie, delete a tie) based on important features and their parameters\nAfter each change, decide: “Is this new network more or less probable given my parameters?”\nKeep changes that lead to more probable networks (and sometimes keep changes that don’t, to explore broadly)\nRepeat thousands of times\n\nAfter many iterations, we’ll have a sample of networks that represents what networks look like under our parameters. We use this sample to estimate the normalizing constant.\nLet’s see this in action with a simple example:\nImagine we have some parameters in mind: β₀ = -3.0 (sparse networks), β₁ = 2.0 (same-party pairs more likely). Let’s manually walk through two MCMC steps.\n\n# Function to calculate network score given parameters\ncalculate_score &lt;- function(net, beta_edges, beta_party) {\n  n_edges &lt;- network.edgecount(net)\n  \n  # Count same-party ties\n  party_vec &lt;- get.vertex.attribute(net, \"party\")\n  n_party_match &lt;- 0\n  for(i in 1:network.size(net)) {\n    for(j in 1:network.size(net)) {\n      if(i != j && net[i,j] == 1) {\n        if(party_vec[i] == party_vec[j]) {\n          n_party_match &lt;- n_party_match + 1\n        }\n      }\n    }\n  }\n  \n  score &lt;- beta_edges * n_edges + beta_party * n_party_match\n  return(list(score = score, edges = n_edges, party_match = n_party_match))\n}\n\n# Our hypothetical parameters\nbeta_edges &lt;- -3.0\nbeta_party &lt;- 2.0\n\nMCMC simulation:\n\n# STEP 0: Calculate score for observed network\ncurrent_net &lt;- house_net\ncurrent_stats &lt;- calculate_score(current_net, beta_edges, beta_party)\n\nStarting with the observed network:\n\nEdges: 57\nSame-party ties: 42\nScore: -87\n\n\n# STEP 1: Propose a change - randomly toggle one tie\nset.seed(111)\n# Pick a random pair\ni &lt;- sample(1:network.size(current_net), 1)\nj &lt;- sample((1:network.size(current_net))[-i], 1)\n\n# Create proposed network\nproposed_net1 &lt;- current_net\nproposed_net1[i, j] &lt;- 1 - proposed_net1[i, j]  # Flip tie (0→1 or 1→0)\n\nproposed_stats1 &lt;- calculate_score(proposed_net1, beta_edges, beta_party)\n\nparty_vec &lt;- get.vertex.attribute(house_net, \"party\")\nchange_type &lt;- ifelse(current_net[i,j] == 0, \"ADD\", \"REMOVE\")\nparty_info &lt;- sprintf(\"(%s-%s)\", party_vec[i], party_vec[j])\n\nPropose to REMOVE 14→20 (R-R). This would result in the following changes to the network:\n\nEdges: 57 → 56\nSame-party ties: 42 → 41\nScore: -87 → -86 (change: 1 point)\n\n\n# Decide whether to accept\nscore_diff &lt;- proposed_stats1$score - current_stats$score\naccept_prob &lt;- min(1, exp(score_diff))\n\nAcceptance probability of 1.\nNext, we take this new network and randomly select a tie to toggle.\n\n# STEP 2: Propose another change\nset.seed(222)\ni &lt;- sample(1:network.size(current_net), 1)\nj &lt;- sample((1:network.size(current_net))[-i], 1)\n\nproposed_net2 &lt;- proposed_net1\nproposed_net2[i, j] &lt;- 1 - proposed_net2[i, j]\n\nproposed_stats2 &lt;- calculate_score(proposed_net2, beta_edges, beta_party)\n\nchange_type &lt;- ifelse(proposed_net1[i,j] == 0, \"ADD\", \"REMOVE\")\nparty_info &lt;- sprintf(\"(%s-%s)\", party_vec[i], party_vec[j])\n\nPropose to REMOVE 15→19 (R-R). This would result in the following changes to the network:\n\nEdges: 56 → 55\nSame-party ties: 41 → 40\nScore: -86 → -85 (change: 1 point)\n\n\nscore_diff &lt;- proposed_stats2$score - proposed_stats1$score\naccept_prob &lt;- min(1, exp(score_diff))\n\nAcceptance probability of 1.\nKey insights:\n\nChanges that INCREASE the score (make network more probable) are always accepted (acceptance probability = 1.0).\nChanges that DECREASE the score might still be accepted with probability = exp(score_change). This prevents getting stuck in local optima.\nAdding a same-party tie increases both edges and party_match. Effect on score: -3.0(+1) + 2.0(+1) = -1.0 (net decrease).\nAdding a cross-party tie increases only edges. Effect on score: -3.0(+1) + 2.0(0) = -3.0 (larger decrease).\nAfter thousands of steps, we’ll have a sample of networks that reflects what networks look like under these parameters.\n\nWhy “Markov Chain”?\nEach new network depends only on the previous network (we make one small change at a time). This creates a “chain” of networks: Network₁ → Network₂ → Network₃ → …\nWhy “Monte Carlo”?\nWe use randomness to explore the space of networks. “Monte Carlo” is a fancy name for “using random sampling to estimate things.”\nThe acceptance rule explained:\n\nIf score increases: Always accept (this network is more probable using the given parameters)\nIf score decreases by Δ: Accept with probability exp(Δ)\n\nSmall decrease: High probability of accepting (explore nearby)\nLarge decrease: Low probability of accepting (avoid improbable networks)\n\n\nThis balance lets us explore the space of probable networks without getting stuck.\n\n\n\n\n\n\nWhy not just randomly sample networks?\n\n\n\nYou might wonder: “Why go through this complicated accept/reject process? Why not just randomly generate networks and use those?”\nHere’s the problem. Imagine our parameters say:\n\nβ₀ = -3.0 (networks should be very sparse)\nβ₁ = 2.0 (same-party ties strongly preferred)\n\nIf we randomly generate networks (flip a coin for each possible tie), we’d get:\n\nNetworks with about 190 ties on average (50% of 380 possible ties)\nThese random networks are EXTREMELY improbable under our parameters\nWe’d waste computational effort evaluating networks that have essentially zero probability\n\nThe key insight: We need to sample networks proportional to their probability.\nA concrete analogy:\nSuppose you want to estimate the average income in a city.\n\nRandom sampling: Survey 1,000 completely random people. This works!\nERGM’s challenge: But imagine 99.9% of people earn $0 and only 0.1% earn normal incomes. Random sampling would very frequently give you all zeros, which isn’t useful for understanding the income distribution.\nMCMC solution: Sample people proportional to their income probability. You’d naturally get more samples from the relevant part of the distribution.\n\nFor ERGMs, the “relevant part” is networks that are probable under our parameters. MCMC’s accept/reject rule ensures we spend our computational effort where it matters—on networks that could actually occur given our model."
  },
  {
    "objectID": "blog/ERGMs_simulation.html#step-4-fitting-a-simple-ergm",
    "href": "blog/ERGMs_simulation.html#step-4-fitting-a-simple-ergm",
    "title": "Fitting ERGMs to Real Networks",
    "section": "Step 4: Fitting a Simple ERGM",
    "text": "Step 4: Fitting a Simple ERGM\nLet’s start with a model that includes both dyadic factors (party) and a structural feature (reciprocity). Because reciprocity creates dependencies between ties, this model requires MCMC—we cannot use simple logistic regression.\n\n# Model 1: Edges + Party homophily + Reciprocity\nmodel1 &lt;- ergm(house_net ~ edges + nodematch(\"party\") + mutual)\n\nsummary(model1)\n\nCall:\nergm(formula = house_net ~ edges + nodematch(\"party\") + mutual)\n\nMonte Carlo Maximum Likelihood Results:\n\n                Estimate Std. Error MCMC % z value Pr(&gt;|z|)    \nedges            -2.5411     0.2725      0  -9.325  &lt; 1e-04 ***\nnodematch.party   1.2238     0.3152      0   3.883 0.000103 ***\nmutual            0.4740     0.5098      0   0.930 0.352510    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 526.8  on 380  degrees of freedom\n Residual Deviance: 301.4  on 377  degrees of freedom\n \nAIC: 307.4  BIC: 319.2  (Smaller is better. MC Std. Err. = 0.07014)\n\n\nUnderstanding the output:\nThe ergm() function uses MCMC to estimate parameters. Let’s interpret what we got:\n\nedges: This is like β₀ (baseline). It tells us the overall tendency to form ties. A negative value means ties are generally sparse.\nnodematch.party: This tells us whether same-party members co-sponsor more. A positive value means “yes, same party increases co-sponsorship.”\nmutual: This tells us about reciprocity. A positive value means co-sponsorship tends to be mutual—if A co-sponsors B’s bills, B is more likely to co-sponsor A’s bills.\n\nWhy this model requires MCMC:\nThe mutual term creates dependency: the probability of tie B→A depends on whether tie A→B exists. This means we cannot treat each tie independently (which would allow simple logistic regression). We must use MCMC to handle this interdependence between a large number of actors.\nWhat happened behind the scenes?\nFitting an ERGM involves finding the parameters (β values) that make our observed network most probable. But we can’t calculate probabilities exactly because we can’t enumerate all networks. So we use an iterative process:\n\nStart with initial parameter guesses (often all zeros: β₀ = 0, β₁ = 0, β₂ = 0, or those estimated using maximum pseudo-likelihood estimation)\nCalculate the sufficient statistics of our observed network:\n\nObserved edges: count them\nObserved same-party ties: count them\nObserved mutual dyads: count them\n\nFor the current parameter values, use MCMC to simulate many networks and calculate their average statistics:\n\nAverage edges across simulated networks\nAverage same-party ties across simulated networks\nAverage mutual dyads across simulated networks\n\nCompare observed statistics to simulated statistics:\n\nIf observed edges &gt; simulated edges: increase β₀ (make edges more likely)\nIf observed edges &lt; simulated edges: decrease β₀ (make edges less likely)\nSame logic for β₁ and same-party ties\nSame logic for β₂ and mutual dyads\n\nUpdate parameters and repeat steps 3-4\nContinue until average of the simulated statistics match observed statistics\n\nWhy this works:\nWhen the parameters are “correct” (maximum likelihood estimates), the simulated networks will have the same statistics on average as our observed network. If observed edges = 50 and simulated edges average to 35, we need to increase β₀. If they match at 50 and 50, we’ve found the right parameter!\nThe computational challenge:\nFor our 20-member network with structural dependencies, the ergm() function might:\n\nTry 20-30 different parameter combinations\nFor each combination, simulate 1,000-10,000 networks\nTotal: 20,000-300,000 network evaluations\n\nThis happens in seconds on modern computers, but would be impossible if we tried to enumerate all \\(2^{380}\\) possible networks!\nA simplified example:\nObserved network statistics:\n\nEdges: 57\nSame-party ties: 42\nMutual dyads: 7\n\nImagine the algorithm tries these parameter values:\n\nIteration 1: β₀ = 0.0, β₁ = 0.0, β₂ = 0.0 (starting guess)\n\nSimulated edges: 95 (too many!)\nSimulated same-party: 48 (may differ from observed)\nSimulated mutual: 12 (may differ from observed)\nTherefore: decrease β₀, adjust β₁ and β₂\n\nIteration 2: β₀ = -1.5, β₁ = 1.0, β₂ = 0.5\n\nSimulated edges: 68 (still too many)\nSimulated same-party: 43 (getting closer)\nSimulated mutual: 6 (adjusting)\nTherefore, decrease β₀ more, adjust others\n\nIteration 3: β₀ = -2.3, β₁ = 1.4, β₂ = 1.2\n\nSimulated edges: close to 57\nSimulated same-party: close to 42\nSimulated mutual: close to 7\nTherefore, small adjustments\n\n\n… (continues until convergence) …\nFinal estimates (from actual model):\n-   β₀ (edges) = -2.541\n-   β₁ (same-party) = 1.224\n-   β₂ (mutual) = 0.474"
  },
  {
    "objectID": "blog/ERGMs_simulation.html#step-5-interpreting-parameters",
    "href": "blog/ERGMs_simulation.html#step-5-interpreting-parameters",
    "title": "Fitting ERGMs to Real Networks",
    "section": "Step 5: Interpreting Parameters",
    "text": "Step 5: Interpreting Parameters\nLet’s interpret our results in plain language:\nEdges coefficient: -2.541.\n\nThis negative number means co-sponsorship ties are sparse.\nBaseline probability of a tie: 0.073, or 7%.\n\nSame-party coefficient: 1.224.\n\nThis positive number means same-party members co-sponsor more.\nMultiplier effect: exp(1.224), or 3.4.\nSame-party pairs are 3.4 times more likely to co-sponsor (compared to the baseline).\n\nMutual coefficient: 0.474.\n\nPositive reciprocity: Co-sponsorship tends to be mutual. This captures the norm of “you scratch my back, I’ll scratch yours” in Congress.\nIf A co-sponsors B’s bills, B is 1.61 times more likely to co-sponsor A’s bills (compared to baseline)."
  },
  {
    "objectID": "blog/ERGMs_simulation.html#step-6-checking-model-fit-with-simulation",
    "href": "blog/ERGMs_simulation.html#step-6-checking-model-fit-with-simulation",
    "title": "Fitting ERGMs to Real Networks",
    "section": "Step 6: Checking Model Fit with Simulation",
    "text": "Step 6: Checking Model Fit with Simulation\nHow do we know if our model is good? We simulate networks from the fitted model and compare them to our observed network.\nThe logic:\n\nOur fitted model has parameters that define a probability distribution over all networks\nWe can use MCMC to draw sample networks from this distribution\nIf our model is good, these simulated networks should look like our observed network\n\n\n# Simulate 100 networks from the fitted model\nsim_networks &lt;- simulate(model1, nsim = 100)\n\n# Calculate statistics for simulated networks\nsim_stats &lt;- attr(sim_networks, \"stats\")\n\n# Compare to observed network statistics\nobs_edges &lt;- summary(house_net ~ edges)\nobs_party &lt;- summary(house_net ~ nodematch(\"party\"))\nobs_mutual &lt;- summary(house_net ~ mutual)\n\nGoodness of fit:\nObserved network statistics:\n\ntibble(\n  Statistic = c(\"Edges\", \"Same-party ties\", \"Mutual dyads\"),\n  Observed = c(summary(house_net ~ edges), summary(house_net ~ nodematch(\"party\")), summary(house_net ~ mutual)),\n  Simulated = c(mean(sim_stats[, \"edges\"]), mean(sim_stats[, \"nodematch.party\"]), mean(sim_stats[, \"mutual\"]))\n) %&gt;% \n  knitr::kable()\n\n\n\n\nStatistic\nObserved\nSimulated\n\n\n\n\nEdges\n57\n58.97\n\n\nSame-party ties\n42\n42.87\n\n\nMutual dyads\n7\n7.76\n\n\n\n\n\nWhat are we checking?\nIf the simulated statistics are close to the observed statistics, our model captures the key features of the network well."
  },
  {
    "objectID": "blog/ERGMs_simulation.html#step-7-adding-transitivity-clustering",
    "href": "blog/ERGMs_simulation.html#step-7-adding-transitivity-clustering",
    "title": "Fitting ERGMs to Real Networks",
    "section": "Step 7: Adding Transitivity (Clustering)",
    "text": "Step 7: Adding Transitivity (Clustering)\nDo House members form tight collaboration clusters? If A co-sponsors with B, and B co-sponsors with C, is A more likely to co-sponsor with C?\nThis is called transitivity or triadic closure—the same structural feature we studied with conflict networks, but here it might be positive (legislators form caucuses and working groups).\n\n# Model 3: Add transitivity\n# Note: gwesp is a \"geometrically weighted edgewise shared partner\" term\n# It's a more stable way to model transitivity than counting all triangles\nmodel2 &lt;- ergm(house_net ~ edges + nodematch(\"party\") + mutual + gwesp(0.5, fixed = TRUE))\n\nsummary(model2)\n\nCall:\nergm(formula = house_net ~ edges + nodematch(\"party\") + mutual + \n    gwesp(0.5, fixed = TRUE))\n\nMonte Carlo Maximum Likelihood Results:\n\n                    Estimate Std. Error MCMC % z value Pr(&gt;|z|)    \nedges                -2.4163     0.3399      0  -7.109  &lt; 1e-04 ***\nnodematch.party       1.2879     0.3588      0   3.589 0.000332 ***\nmutual                0.4578     0.5130      0   0.892 0.372228    \ngwesp.OTP.fixed.0.5  -0.1585     0.2351      0  -0.674 0.500091    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 526.8  on 380  degrees of freedom\n Residual Deviance: 301.3  on 376  degrees of freedom\n \nAIC: 309.3  BIC: 325  (Smaller is better. MC Std. Err. = 0.07575)\n\n\nUnderstanding gwesp:\nThe gwesp term measures transitivity in a sophisticated way. Instead of simply counting triangles (which can cause computational problems), it weights different types of shared partnerships.\n\nPositive gwesp: Legislators form clusters (if A↔︎B and B↔︎C, then A↔︎C is more likely)\nNegative gwesp: Legislators avoid complex collaboration patterns\n\nGWESP coefficient: -0.159.\n\nNegative transitivity: Legislators avoid dense collaboration clusters.\nHowever, this coefficient is not statistically significant.\n\nThis makes sense: the only feature we included when we created this network was party matching."
  },
  {
    "objectID": "blog/ERGMs_simulation.html#step-8-understanding-mcmc-diagnostics",
    "href": "blog/ERGMs_simulation.html#step-8-understanding-mcmc-diagnostics",
    "title": "Fitting ERGMs to Real Networks",
    "section": "Step 8: Understanding MCMC Diagnostics",
    "text": "Step 8: Understanding MCMC Diagnostics\nWhen fitting ERGMs with MCMC, we need to check that the simulation converged properly.\nWhat does “convergence” mean?\nImagine you’re exploring a vast landscape in the fog, trying to map out the terrain. You take random steps and record what you find. “Convergence” means you’ve explored enough that you have a good sense of the whole landscape—you’re not stuck in one corner or wandering aimlessly.\nFor MCMC, convergence means:\n\nThe chain has explored the space of likely networks adequately\nThe parameter estimates have stabilized\nWe can trust the results\n\nWe can check for convergence in the long output provided by mcmc.diagnostics():\n\n# Check MCMC diagnostics\nmcmc.diagnostics(model2)\n\n\n\n\n\n\n\n\nSample statistics summary:\n\nIterations = 14336:262144\nThinning interval = 1024 \nNumber of chains = 1 \nSample size per chain = 243 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n                       Mean    SD Naive SE Time-series SE\nedges               -0.1893 6.832   0.4383         0.4496\nnodematch.party     -0.1152 5.691   0.3651         0.4338\nmutual              -0.2181 2.535   0.1626         0.1983\ngwesp.OTP.fixed.0.5  1.5174 8.169   0.5240         0.5240\n\n2. Quantiles for each variable:\n\n                      2.5%    25%     50%   75% 97.5%\nedges               -13.00 -5.000 -1.0000 5.000 12.95\nnodematch.party     -10.00 -4.000  0.0000 4.000 11.00\nmutual               -4.00 -2.000  0.0000 1.000  5.00\ngwesp.OTP.fixed.0.5 -11.37 -4.713  0.3935 7.074 18.35\n\n\nAre sample statistics significantly different from observed?\n                edges nodematch.party     mutual gwesp.OTP.fixed.0.5\ndiff.      -0.1893004      -0.1152263 -0.2181070         1.517376917\ntest stat. -0.4210715      -0.2655990 -1.0998290         2.895541986\nP-val.      0.6737029       0.7905481  0.2714067         0.003785044\n                 (Omni)\ndiff.                NA\ntest stat. 3.971829e+01\nP-val.     2.356427e-07\n\nSample statistics cross-correlations:\n                        edges nodematch.party    mutual gwesp.OTP.fixed.0.5\nedges               1.0000000       0.8499789 0.5851644           0.8455051\nnodematch.party     0.8499789       1.0000000 0.6271511           0.7745410\nmutual              0.5851644       0.6271511 1.0000000           0.5053557\ngwesp.OTP.fixed.0.5 0.8455051       0.7745410 0.5053557           1.0000000\n\nSample statistics auto-correlation:\nChain 1 \n                edges nodematch.party      mutual gwesp.OTP.fixed.0.5\nLag 0     1.000000000      1.00000000  1.00000000         1.000000000\nLag 1024  0.093994006      0.18212609  0.19376684         0.086520670\nLag 2048 -0.004748367      0.07115234 -0.01070746         0.001059334\nLag 3072  0.081117030      0.11161007  0.09712974         0.012278959\nLag 4096 -0.128161317     -0.10894714 -0.03885801        -0.120461001\nLag 5120 -0.042044941     -0.08834113 -0.03245954        -0.051045473\n\nSample statistics burn-in diagnostic (Geweke):\nChain 1 \n\nFraction in 1st window = 0.1\nFraction in 2nd window = 0.5 \n\n              edges     nodematch.party              mutual gwesp.OTP.fixed.0.5 \n          0.9022633           0.6309148           0.9914133           1.0159278 \n\nIndividual P-values (lower = worse):\n              edges     nodematch.party              mutual gwesp.OTP.fixed.0.5 \n          0.3669170           0.5280962           0.3214838           0.3096637 \nJoint P-value (lower = worse):  0.8258765 \n\nNote: MCMC diagnostics shown here are from the last round of\n  simulation, prior to computation of final parameter estimates.\n  Because the final estimates are refinements of those used for this\n  simulation run, these diagnostics may understate model performance.\n  To directly assess the performance of the final model on in-model\n  statistics, please use the GOF command: gof(ergmFitObject,\n  GOF=~model).\n\n\nSample statistics summary interpretation:\n\nIterations = 14336:262144: This is the range of iterations that were used to calculate the reported sample statistics.\n\nThe statistics summarized in the diagnostic report were calculated using the 247,809 samples collected from iteration 14,336 up to and including 262,144.\nThe MCMC sampler ran for 14,335 iterations before the burn-in period ended and the sampling for statistics began.\n\nThinning interval = 1024: The MCMC algorithm only records the state of the network once every 1,024 steps/iterations.\n\nWhy? To obtain a set of recorded samples that are as close to independent as possible. Successive states of an MCMC chain are often highly correlated (i.e., the state at iteration \\(t\\) is very similar to the state at iteration \\(t+1\\)). High autocorrelation means the chain is exploring the sample space slowly, and the samples provide redundant information. By skipping 1,023 steps between each recorded sample, the chain has more time to move to a statistically independent region of the distribution space.\n\nNumber of chains = 1: Only one MCMC simulation was executed and its output was used for the entire analysis.\nSample size per chain = 243: The final, usable number of independent samples that were recorded for the purpose of inference and diagnostics after accounting for burn-in and thinning.\n\nSample statistic distributions:\nThe empirical mean and standard deviation for each variable, plus standard error of the mean shows the center and spread of differences between the statistics for each 243 sampled networks and the observed network. For example, a mean of -0.1893 for the edges variable indicates that the sampled networks had, on average, 0.1893 fewer edges than the observed network.\nThe diagnostic summary then asks “Are sample statistics significantly different from observed?” Ideally, they are not (we get large test statistics and p-values). This is the case for all of our variables except for transitivity, which is reflected in its non-significance in our model.\nThe summary also includes sample statistics auto-correlation across all chains (here, our one chain). Cross-correlation measures the correlation between the sequence of samples for statistic \\(A\\) and the sequence of samples for statistic \\(B\\). High positive cross-correlation means that as the MCMC chain moves to a network state with a high value of statistic A, it also tends to move to a network state with a high value of statistic B. The two statistics move in tandem. For example, edges aand nodematch.party are highly positively correlated. This means that the chain struggles to explore networks with many edges but few same party matches, or vice-versa. Warning: high cross-correlations in the sample statistics are a sign of poor mixing and can lead to degeneracy issues in the model.\nFinally, the summary includes – in the Geweke diagnostic – a measure of whether the MCMC chain has reached its stationary distribution and is therefore sampling from the target distribution (the ERGM model space). It helps determine if the specified burn-in period was sufficient. This works by dividing the chain into different segments and then asking if the mean parameters of each section are statistically significantly different from each other. The first set of numbers (for example, edges: 0.9022633) is the mean parameter’s Z-score. The second set of numbers (for example, edges: 0.3669170) is the corresponding p-value. Ideally, the segments are not significantly different from each other.\nIf diagnostics look good, we can trust our parameter estimates!"
  },
  {
    "objectID": "blog/ERGMs_simulation.html#step-9-comparing-models",
    "href": "blog/ERGMs_simulation.html#step-9-comparing-models",
    "title": "Fitting ERGMs to Real Networks",
    "section": "Step 9: Comparing Models",
    "text": "Step 9: Comparing Models\nWhich model best explains co-sponsorship patterns? We can compare models using information criteria:\n\n# Compare models using AIC (Akaike Information Criterion)\n# Lower AIC = better model\n\naic1 &lt;- AIC(model1)\naic2 &lt;- AIC(model2)\n\ntibble(\n  model = c(\"Model 1 (edges + party)\", \"Model 3 (+ transitivity)\"),\n  AIC = c(aic1, aic2)\n) %&gt;% \n  knitr::kable()\n\n\n\n\nmodel\nAIC\n\n\n\n\nModel 1 (edges + party)\n307.3889\n\n\nModel 3 (+ transitivity)\n309.2818\n\n\n\n\n\nThe best model according to this measure is model 1.\nInterpreting AIC:\n\nLower AIC = better balance of fit and complexity\nA difference of 2+ points suggests meaningful improvement\nThe best model captures the data patterns most efficiently"
  },
  {
    "objectID": "blog/ERGMs_simulation.html#the-power-of-mcmc",
    "href": "blog/ERGMs_simulation.html#the-power-of-mcmc",
    "title": "Fitting ERGMs to Real Networks",
    "section": "The Power of MCMC",
    "text": "The Power of MCMC\nWhat we accomplished:\nInstead of enumerating 2^380 impossible networks, we:\n\nUsed simulation to estimate the normalizing constant\nFitted complex models with structural dependencies\nTested hypotheses about party, reciprocity, and clustering\nAssessed model fit through simulation\n\nWhy this matters:\nMCMC makes ERGMs practical for real political networks. We can now study:\n\nLegislative collaboration in Congress (435 members)\nInternational alliance networks (195 countries)\nState-level policy diffusion (50 states)\nSocial movements and organizational ties\n\nThe logic remains the same as our 3-node example—we’re modeling probability distributions over all possible networks that could form between our actors. MCMC just gives us the computational tools to apply this logic to realistic political science questions."
  },
  {
    "objectID": "blog/ERGMs_simulation.html#key-takeaways",
    "href": "blog/ERGMs_simulation.html#key-takeaways",
    "title": "Fitting ERGMs to Real Networks",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nFrom enumeration to simulation:\n\nSmall networks: Calculate probabilities exactly by enumerating all possibilities\nLarge networks: Use MCMC to estimate probabilities through simulation\n\nThe ERGM framework stays the same:\n\nParameters (β values) weight different features\nNetworks with more “desirable” features get higher probability\nWe find parameters that make our observed network most likely\n\nWhat MCMC adds:\n\nComputational feasibility for real networks\nAbility to include complex structural features\nGoodness-of-fit testing through simulation\nModel comparison tools\n\nThe fundamental insight:\nPolitical relationships are interdependent. ERGMs with MCMC let us rigorously study these interdependencies in real-world networks—whether studying how House members form legislative coalitions, how countries build alliance networks, or how policies diffuse across states."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Harriet Goers",
    "section": "",
    "text": "I am a PhD student in the Department of Government and Politics at the University of Maryland, College Park. I research how small- and middle-sized countries leverage their economic power. I am also an adjunct with the RAND Corporation, where I research critical minerals supply chain vulnerabilities and opportunities for international cooperation in this space.Prior to starting my PhD, I worked on the Global Peace Index with the Institute for Economics and Peace, as an analyst with the Australian Department of Defence, and on the Asia Power Index with the Lowy Institute."
  }
]